# -*- coding: utf-8 -*-
"""1i ergasia_fromscratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12nrpivbuD3BRsCFpiW6Vap4dCLgQNab4
"""

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import cifar10

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.reshape(x_train.shape[0], -1).astype('float32') / 255.0
x_test = x_test.reshape(x_test.shape[0], -1).astype('float32') / 255.0

class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

input_size = x_train.shape[1]  # 3072 (32x32x3)
hidden_size_1 = 128
hidden_size_2 = 64
output_size = 10
learning_rate = 0.01
epochs = 50
batch_size = 128

# weights and biases initialisation
W1 = np.random.randn(input_size, hidden_size_1) * 0.01
b1 = np.zeros((1, hidden_size_1))
W2 = np.random.randn(hidden_size_1, hidden_size_2) * 0.01
b2 = np.zeros((1, hidden_size_2))
W3 = np.random.randn(hidden_size_2, output_size) * 0.01
b3 = np.zeros((1, output_size))

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def forward_pass(x):
    global Z1, A1, Z2, A2, Z3, A3
    Z1 = np.dot(x, W1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = relu(Z2)
    Z3 = np.dot(A2, W3) + b3
    A3 = softmax(Z3)
    return A3

# cross-entropy loss
def compute_loss(output, y_true):
    m = y_true.shape[0]
    correct_class_probabilities = output[np.arange(m), y_true.flatten()]
    loss = -np.sum(np.log(correct_class_probabilities)) / m
    return loss

# backpropagation
def backward_pass(x, y, output):
    global W1, b1, W2, b2, W3, b3
    m = x.shape[0]

    # computing gradients
    dZ3 = output
    dZ3[np.arange(m), y.flatten()] -= 1
    dZ3 /= m
    dW3 = np.dot(A2.T, dZ3)
    db3 = np.sum(dZ3, axis=0, keepdims=True)

    dA2 = np.dot(dZ3, W3.T)
    dZ2 = dA2 * relu_derivative(Z2)
    dW2 = np.dot(A1.T, dZ2)
    db2 = np.sum(dZ2, axis=0, keepdims=True)

    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * relu_derivative(Z1)
    dW1 = np.dot(x.T, dZ1)
    db1 = np.sum(dZ1, axis=0, keepdims=True)

    # updating weights and biases
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W3 -= learning_rate * dW3
    b3 -= learning_rate * db3

# per-class accuracy
def print_per_class_accuracy(y_true, y_pred):
    classes = np.unique(y_true)
    print("\nPer-class accuracy:")
    for i, class_name in enumerate(class_names):
        mask = (y_true.flatten() == i)
        class_acc = np.mean(y_pred[mask] == y_true.flatten()[mask]) * 100
        print(f"{class_name}: {class_acc:.2f}%")
        
# we will store the accuracies here
train_accuracies = []
test_accuracies = []

print("\nTraining the model...")
for epoch in range(epochs):
    for i in range(0, x_train.shape[0], batch_size):
        x_batch = x_train[i:i + batch_size]
        y_batch = y_train[i:i + batch_size]

        # FP
        output = forward_pass(x_batch)
        loss = compute_loss(output, y_batch)
        # BP
        backward_pass(x_batch, y_batch, output)

    # evaluation at the training set
    train_predictions = np.argmax(forward_pass(x_train), axis=1)
    train_accuracy = np.mean(train_predictions == y_train.flatten()) * 100
    train_accuracies.append(train_accuracy)

    # evaluation at the test set
    test_predictions = np.argmax(forward_pass(x_test), axis=1)
    test_accuracy = np.mean(test_predictions == y_test.flatten()) * 100
    test_accuracies.append(test_accuracy)

    print(f"Epoch {epoch + 1}/{epochs} - Loss: {loss:.4f} - Train Accuracy: {train_accuracy:.2f}% - Test Accuracy: {test_accuracy:.2f}%")

print(f"Final Test Accuracy: {test_accuracies[-1]:.2f}%")
print_per_class_accuracy(y_test, test_predictions)

plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')
plt.plot(range(1, epochs + 1), test_accuracies, label='Testing Accuracy')
plt.title('Training and Testing Accuracy over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True)
plt.show()